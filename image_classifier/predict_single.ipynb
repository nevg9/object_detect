{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torchvision  # scope for faster 'import ultralytics'\n",
    "from pathlib import Path\n",
    "from ultralytics.data.dataset import classify_transforms\n",
    "from ultralytics.nn.tasks import torch_safe_load, ClassificationModel\n",
    "import torch\n",
    "from safetensors.torch import load_file\n",
    "from tqdm import tqdm\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = torchvision.datasets.ImageFolder(root=\"/home/yuzhong/data1/image_classifier_data/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_to_class_names = {}\n",
    "\n",
    "for imfile, id in base.samples:\n",
    "    class_name = imfile.split('/')[-2]\n",
    "    if id in id_to_class_names:\n",
    "        assert id_to_class_names[id] == class_name\n",
    "    else:\n",
    "        id_to_class_names[id] = class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'animal', 1: 'human', 2: 'no_target'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predict_image(root, imgsz):\n",
    "    base = torchvision.datasets.ImageFolder(root=root)\n",
    "    samples = base.samples\n",
    "    torch_transforms = classify_transforms(size=imgsz)\n",
    "    for file_name, class_id in samples:\n",
    "        im = cv2.imread(file_name)\n",
    "        im = Image.fromarray(cv2.cvtColor(im, cv2.COLOR_BGR2RGB))\n",
    "        sample = torch_transforms(im)\n",
    "        yield {\"img\": sample, \"cls\": class_id}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_yolo_model(path, nc):\n",
    "    ckpt, w = torch_safe_load(path)\n",
    "    model = ckpt[\"model\"]\n",
    "    ClassificationModel.reshape_outputs(model, nc)\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False  # for training\n",
    "    model = model.float()\n",
    "    return model\n",
    "\n",
    "model = load_yolo_model(\"/home/yuzhong/data1/models/yolo/yolov8m-cls.pt\", 3)\n",
    "weights = load_file('/home/yuzhong/data1/code/object_detection/image_classifier/epoch_31/model.safetensors')\n",
    "model.load_state_dict(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_images(model, image_paths, imgsz, device_id=0):\n",
    "    model.eval()\n",
    "    model.to(device_id)\n",
    "    predict_class = []\n",
    "    ground_truth_class = []\n",
    "    with torch.no_grad():\n",
    "        for im_info in tqdm(get_predict_image(image_paths, imgsz), desc=\"predict image\"):\n",
    "            im = im_info['img'].unsqueeze(0).to(device_id)\n",
    "            class_id = im_info['cls']\n",
    "            ground_truth_class.append(class_id)\n",
    "            outputs = model(im)\n",
    "            predictions = outputs.argmax(dim=-1)\n",
    "            predict_class.append(predictions.item())\n",
    "    return predict_class, ground_truth_class\n",
    "\n",
    "def predict_batch_images(model, image_paths, imgsz, device_id=0, batch_size=32):\n",
    "    model.eval()\n",
    "    model.to(device_id)\n",
    "    predict_class = []\n",
    "    ground_truth_class = []\n",
    "    \n",
    "    batch_images = []\n",
    "    batch_classes = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for im_info in tqdm(get_predict_image(image_paths, imgsz), desc=\"predict image\"):\n",
    "            im = im_info['img']\n",
    "            class_id = im_info['cls']\n",
    "            \n",
    "            batch_images.append(im)\n",
    "            batch_classes.append(class_id)\n",
    "            \n",
    "            if len(batch_images) == batch_size:\n",
    "                batch_tensor = torch.stack(batch_images).to(device_id)\n",
    "                outputs = model(batch_tensor)\n",
    "                predictions = outputs.argmax(dim=-1)\n",
    "                \n",
    "                # Store predictions and ground truth\n",
    "                predict_class.extend(predictions.cpu().numpy().tolist())\n",
    "                ground_truth_class.extend(batch_classes)\n",
    "                \n",
    "                # Clear the batch lists\n",
    "                batch_images = []\n",
    "                batch_classes = []\n",
    "        \n",
    "        # Process any remaining images in the batch (if total % batch_size != 0)\n",
    "        if batch_images:\n",
    "            batch_tensor = torch.stack(batch_images).to(device_id)\n",
    "            outputs = model(batch_tensor)\n",
    "            predictions = outputs.argmax(dim=-1)\n",
    "            \n",
    "            predict_class.extend(predictions.cpu().numpy().tolist())\n",
    "            ground_truth_class.extend(batch_classes)\n",
    "    \n",
    "    return predict_class, ground_truth_class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "predict image: 251286it [8:03:57,  8.65it/s]\n"
     ]
    }
   ],
   "source": [
    "predict_class, ground_truth_class = predict_batch_images(model, \"/home/yuzhong/data1/image_classifier_data/test\", 224, batch_size=1280)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251286"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predict_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "251286"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ground_truth_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.add_batch(\n",
    "                predictions=predict_class,\n",
    "                references=ground_truth_class,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_metric = metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.944843723884339}\n"
     ]
    }
   ],
   "source": [
    "print(eval_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
